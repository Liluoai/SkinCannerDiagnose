{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure\n",
    "\n",
    "I will use the head of ResNet50, Xception, VGG19, VGG16 to get the feature vectors of the input image, and combine those feature vectors to a new feature vector. Then add a dropout and a dense layer behind the feature vector.\n",
    "\n",
    "![图片](model_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "              \n",
    "train_dataset_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip'\n",
    "valid_dataset_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip'\n",
    "test_dataset_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip'\n",
    "              \n",
    "print('start downloading train dataset')\n",
    "request.urlretrieve(train_dataset_url, 'train.zip')\n",
    "print('start downloading valid dataset')\n",
    "request.urlretrieve(valid_dataset_url, 'valid.zip')\n",
    "print('start downloading test dataset')\n",
    "request.urlretrieve(test_dataset_url, 'test.zip')\n",
    "print('download finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "train_zf = zipfile.ZipFile('train.zip','r')\n",
    "train_zf.extractall(path = 'dataset/train')\n",
    "valid_zf = zipfile.ZipFile('valid.zip','r')\n",
    "valid_zf.extractall(path = 'dataset/valid')\n",
    "test_zf = zipfile.ZipFile('test.zip','r')\n",
    "test_zf.extractall(path = 'dataset/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Feature Vectors To Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from sklearn.utils import shuffle\n",
    "import cv2, os, itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'data/train/'\n",
    "TEST_DIR = 'data/test/'\n",
    "\n",
    "train_dir = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR)]\n",
    "test_dir = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "train_dir.sort()\n",
    "test_dir.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_feature_vector = np.ndarray((len(train_dir), 2048 + 2048 + 512 + 512), dtype = np.float32)\n",
    "concat_test_feature_vector = np.ndarray((len(test_dir), 2048 + 2048 + 512 + 512), dtype = np.float32)\n",
    "\n",
    "def get_feature_vector_list(image_path_list, MODEL, input_size, preprocess_fun = None):\n",
    "    inputs = Input(input_size)\n",
    "    if preprocess_fun:\n",
    "        inputs = Lambda(preprocess_fun)(inputs)\n",
    "        \n",
    "    base_model = MODEL(input_tensor = inputs, weights = 'imagenet', include_top = False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "\n",
    "    print('start {} prediction:'.format(base_model.name))\n",
    "    feature_vector_list = []\n",
    "    for i, image_path in enumerate(image_path_list):\n",
    "        input_image = prepare_data(image_path, input_size)\n",
    "        input_image = np.expand_dims(input_image, axis = 0)\n",
    "        feature_vector = model.predict(input_image, verbose = 0)\n",
    "        feature_vector_list.append(feature_vector)\n",
    "        # show progress bar\n",
    "        if i%(len(image_path_list)//100) == 0:\n",
    "            print('>', end = '')\n",
    "    print('finish {} prediction'.format(base_model.name))\n",
    "    \n",
    "    return feature_vector_list\n",
    "\n",
    "        \n",
    "def prepare_data(image_path, input_size):\n",
    "    rows = input_size[0]\n",
    "    cols = input_size[1]\n",
    "    channels = input_size[2]\n",
    "    data = np.ndarray(input_size, dtype = np.uint8)\n",
    "    \n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (rows, cols), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def get_and_save_feature_vector():\n",
    "    #ResNet50 output: (1, 2048)\n",
    "    ResNet50_train_feature_vector_list = get_feature_vector_list(train_dir, ResNet50, (224, 224, 3))\n",
    "    #Xception output: (1, 2048)\n",
    "    Xception_train_feature_vector_list = get_feature_vector_list(train_dir, Xception, (299, 299, 3), xception.preprocess_input)\n",
    "    #TODO\n",
    "    # get_feature_vector(image_path, InceptionV3, (299, 299, 3), inception_v3.preprocess_input)\n",
    "    #VGG16 output: (1, 512)\n",
    "    VGG16_train_feature_vector_list = get_feature_vector_list(train_dir, VGG16, (224, 224, 3))\n",
    "    #VGG19 output: (1, 512)\n",
    "    VGG19_train_feature_vector_list = get_feature_vector_list(train_dir, VGG19, (224, 224, 3))\n",
    "    for i in range(len(train_dir)):\n",
    "        concat_train_feature_vector[i] = np.concatenate([ResNet50_train_feature_vector_list[i], \n",
    "                                                        Xception_train_feature_vector_list[i],\n",
    "                                                        VGG16_train_feature_vector_list[i],\n",
    "                                                        VGG19_train_feature_vector_list[i]], axis=1)\n",
    "    \n",
    "    ResNet50_test_feature_vector_list = get_feature_vector_list(test_dir, ResNet50, (224, 224, 3))\n",
    "    Xception_test_feature_vector_list = get_feature_vector_list(test_dir, Xception, (299, 299, 3), xception.preprocess_input)\n",
    "    VGG16_test_feature_vector_list = get_feature_vector_list(test_dir, VGG16, (224, 224, 3))\n",
    "    VGG19_test_feature_vector_list = get_feature_vector_list(test_dir, VGG19, (224, 224, 3))\n",
    "    for i in range(len(test_dir)):\n",
    "        concat_test_feature_vector[i] = np.concatenate([ResNet50_test_feature_vector_list[i], \n",
    "                                                        Xception_test_feature_vector_list[i],\n",
    "                                                        VGG16_test_feature_vector_list[i],\n",
    "                                                        VGG19_test_feature_vector_list[i]], axis=1)\n",
    "    \n",
    "    np.savetxt(\"concat_train_feature_vector.npy\", concat_train_feature_vector, delimiter = ',')\n",
    "    np.savetxt(\"concat_test_feature_vector.npy\", concat_test_feature_vector, delimiter = ',')\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "get_and_save_feature_vector() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Feature Vectors From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_test_feature_vector = np.loadtxt(open(\"concat_test_feature_vector.npy\",\"rb\"), delimiter=\",\").astype(np.float32)\n",
    "concat_train_feature_vector = np.loadtxt(open(\"concat_train_feature_vector.npy\",\"rb\"), delimiter=\",\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Labels And Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for train_image_dir in train_dir:\n",
    "    if 'dog' in train_image_dir:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        \n",
    "y_train = np.array(labels)\n",
    "x_train, y_train = shuffle(concat_train_feature_vector, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define New model, Train And Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (x_train.shape[1], ))\n",
    "x = Dropout(0.5)(inputs)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs = inputs, outputs = predictions)\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, nb_epoch=10, validation_split=0.2)\n",
    "y_pre = model.predict(concat_test_feature_vector)\n",
    "# y_pre = y_pre.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Result To File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def sort_y_pre_by_id(y_pre, test_dir):\n",
    "    y_pre_order_by_id = np.zeros((len(y_pre),))\n",
    "    for i in range(len(test_dir)):\n",
    "        idx = int(test_dir[i][10:-4]) - 1\n",
    "        y_pre_order_by_id[idx] = y_pre[i]\n",
    "    return y_pre_order_by_id\n",
    "\n",
    "y_pre_order_by_id = sort_y_pre_by_id(y_pre, test_dir)\n",
    "\n",
    "with open(\"submission.csv\",\"w\") as csvfile: \n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"id\",\"label\"])\n",
    "    for i in range(len(y_pre_order_by_id)):\n",
    "        writer.writerow([i + 1, y_pre_order_by_id[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have A Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "figure, axs = plt.subplots(3, 3)\n",
    "figure.set_size_inches(15, 15)\n",
    "for i in range(0,3):\n",
    "    for j in range(0, 3):\n",
    "        axs[i][j].set_xticks([])\n",
    "        axs[i][j].set_yticks([])\n",
    "        idx = i * 3 + j\n",
    "        image = cv2.imread(test_dir[idx], cv2.IMREAD_COLOR)\n",
    "        axs[i][j].imshow(image)\n",
    "        \n",
    "        if y_pre[idx] >= 0.5:\n",
    "            title = str(round((float(y_pre[idx]) * 100), 2)) + '%' + 'Dog'\n",
    "            axs[i][j].set_title(title)\n",
    "        else:\n",
    "            title = str(round((100 - float(y_pre[idx]) * 100), 2)) + '%' + 'Cat'\n",
    "            axs[i][j].set_title(title)\n",
    "\n",
    "            \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
